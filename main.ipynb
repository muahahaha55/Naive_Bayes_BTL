{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***D·ª± ƒëo√°n h√†nh vi mua h√†ng c·ªßa ng∆∞·ªùi ti√™u d√πng***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML - NAIVE BAYES V·ªöI OVERSAMPLING (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I.** Kh√°i qu√°t v·ªÅ d·ª± √°n:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1.** B·ªëi c·∫£nh:\n",
    "Trong th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠, vi·ªác hi·ªÉu h√†nh vi ng∆∞·ªùi d√πng gi√∫p doanh nghi·ªáp t·ªëi ∆∞u qu·∫£ng c√°o, tƒÉng t·ª∑ l·ªá chuy·ªÉn ƒë·ªïi v√† doanh thu. D·ª± √°n n√†y s·ª≠ d·ª•ng d·ªØ li·ªáu h√†nh vi ng∆∞·ªùi d√πng ƒë·ªÉ d·ª± ƒëo√°n kh·∫£ nƒÉng mua h√†ng.\n",
    "### **1.2.** M·ª•c ti√™u:\n",
    "- X√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n x√°c su·∫•t ng∆∞·ªùi d√πng mua h√†ng d·ª±a tr√™n c√°c ƒë·∫∑c ƒëi·ªÉm h√†nh vi v√† s·∫£n ph·∫©m.  \n",
    "- Ph√¢n lo·∫°i h√†nh vi:\n",
    "    - 0 ‚Üí Kh√¥ng mua  \n",
    "    - 1 ‚Üí Mua h√†ng\n",
    "### **1.3.** V·∫•n ƒë·ªÅ m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu:\n",
    "Trong th·ª±c t·∫ø, s·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng th·ª±c hi·ªán h√†nh vi mua h√†ng (`purchase`) √≠t h∆°n r·∫•t nhi·ªÅu so v·ªõi ch·ªâ xem (`view`). ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn t·∫≠p d·ªØ li·ªáu b·ªã **m·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng**, khi·∫øn m√¥ h√¨nh c√≥ xu h∆∞·ªõng d·ª± ƒëo√°n theo l·ªõp ƒëa s·ªë (kh√¥ng mua). ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y, ch√∫ng ta s·∫Ω √°p d·ª•ng k·ªπ thu·∫≠t **Oversampling (SMOTE)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **II.** Import th∆∞ vi·ªán v√† ƒë·ªçc d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd     \n",
    "import numpy as np      \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split   \n",
    "from sklearn.naive_bayes import GaussianNB             \n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "pd.set_option('display.max_columns', 20)   \n",
    "pd.set_option('display.width', 1000)       \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'D:\\2019-Nov-100k.csv'\n",
    "#data_path = '2019-Nov-100k.csv' # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n n·∫øu c·∫ßn\n",
    "df = pd.read_csv(data_path, nrows=100000)\n",
    "print(f\"‚úì ƒê√£ ƒë·ªçc d·ªØ li·ªáu th√†nh c√¥ng v·ªõi {len(df):,} d√≤ng v√† {df.shape[1]} c·ªôt.\")\n",
    "print(\"C√°c c·ªôt c√≥ trong t·∫≠p d·ªØ li·ªáu: \")\n",
    "print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III.** Kh√°m ph√° d·ªØ li·ªáu ban ƒë·∫ßu (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Th√¥ng tin chi ti·∫øt v·ªÅ t·∫≠p d·ªØ li·ªáu: \")\n",
    "df.info()\n",
    "print(\"\\nTh·ªëng k√™ m√¥ t·∫£ cho c√°c c·ªôt s·ªë: \")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüõí Th·ªëng k√™ c√°c lo·∫°i h√†nh vi trong t·∫≠p d·ªØ li·ªáu: \")\n",
    "event_counts = df['event_type'].value_counts()\n",
    "print(event_counts)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = event_counts.index, y = event_counts.values, palette='PuBuGn')\n",
    "plt.title(\"Ph√¢n ph·ªëi c√°c lo·∫°i h√†nh vi\")\n",
    "plt.xlabel(\"Lo·∫°i h√†nh vi\")\n",
    "plt.ylabel(\"S·ªë l∆∞·ª£ng\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IV.** L√†m s·∫°ch v√† Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Th·ªëng k√™ gi√° s·∫£n ph·∫©m: \")\n",
    "display(df['price'].describe())\n",
    "plt.boxplot(df['price'], vert=False)\n",
    "plt.title(\"Ph√¢n ph·ªëi gi√° s·∫£n ph·∫©m\")\n",
    "plt.xlabel(\"Gi√°\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X·ª≠ l√Ω gi√° tr·ªã thi·∫øu\n",
    "print(\"--- X·ª≠ l√Ω gi√° tr·ªã thi·∫øu ---\")\n",
    "df['price'] = df['price'].fillna(df['price'].median())\n",
    "df['category_code'] = df['category_code'].fillna('unknown')\n",
    "df['brand'] = df['brand'].fillna('unknown')\n",
    "print(\"‚úì ƒê√£ x·ª≠ l√Ω xong gi√° tr·ªã thi·∫øu.\")\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi c·ªôt th·ªùi gian v√† t·∫°o ƒë·∫∑c tr∆∞ng m·ªõi\n",
    "print(\"\\n--- X·ª≠ l√Ω c·ªôt th·ªùi gian ---\")\n",
    "df['event_time'] = pd.to_datetime(df['event_time'], errors='coerce')\n",
    "df['hour'] = df['event_time'].dt.hour\n",
    "df['day'] = df['event_time'].dt.day\n",
    "df['weekday'] = df['event_time'].dt.weekday\n",
    "print(\"‚úì ƒê√£ t·∫°o c√°c ƒë·∫∑c tr∆∞ng hour, day, weekday.\")\n",
    "\n",
    "# X·ª≠ l√Ω tr√πng l·∫∑p\n",
    "print(\"\\n--- X·ª≠ l√Ω tr√πng l·∫∑p ---\")\n",
    "duplicates_before = df.duplicated().sum()\n",
    "df = df.drop_duplicates()\n",
    "print(f\"‚úì ƒê√£ lo·∫°i b·ªè {duplicates_before} d√≤ng tr√πng l·∫∑p.\")\n",
    "\n",
    "# X·ª≠ l√Ω gi√° tr·ªã ngo·∫°i lai (outliers) trong c·ªôt 'price'\n",
    "print(\"\\n--- X·ª≠ l√Ω gi√° tr·ªã ngo·∫°i lai ---\")\n",
    "Q1 = df['price'].quantile(0.25)\n",
    "Q3 = df['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers_count = df[(df['price'] < lower_bound) | (df['price'] > upper_bound)].shape[0]\n",
    "df = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "print(f\"‚úì ƒê√£ lo·∫°i b·ªè {outliers_count} gi√° tr·ªã ngo·∫°i lai trong c·ªôt 'price'.\")\n",
    "print(f\"\\nK√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi l√†m s·∫°ch: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **V.** Chu·∫©n b·ªã d·ªØ li·ªáu v√† chia Train-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t ƒë·ªÉ tr√°nh r√≤ r·ªâ d·ªØ li·ªáu. Ch√∫ng ta s·∫Ω chia d·ªØ li·ªáu th√¥ (sau khi l√†m s·∫°ch c∆° b·∫£n) th√†nh c√°c t·∫≠p train v√† test tr∆∞·ªõc khi th·ª±c hi·ªán b·∫•t k·ª≥ b∆∞·ªõc m√£ h√≥a ho·∫∑c chu·∫©n h√≥a n√†o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o bi·∫øn m·ª•c ti√™u\n",
    "# L∆∞u √Ω: Ch√∫ng ta t·∫°o c·ªôt 'is_purchase' t·ª´ 'event_type', \n",
    "# do ƒë√≥ 'event_type' kh√¥ng ƒë∆∞·ª£c d√πng l√†m feature n·ªØa.\n",
    "df['is_purchase'] = (df['event_type'] == 'purchase').astype(int)\n",
    "\n",
    "# Lo·∫°i b·ªè 'event_type' kh·ªèi danh s√°ch features ƒë·ªÉ tr√°nh r√≤ r·ªâ d·ªØ li·ªáu\n",
    "features = ['category_code', 'brand', 'price', 'hour', 'day', 'weekday']\n",
    "\n",
    "X = df[features]\n",
    "y = df['is_purchase']\n",
    "\n",
    "# Chia d·ªØ li·ªáu TR∆Ø·ªöC KHI x·ª≠ l√Ω\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (X_train): {X_train.shape}\")\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm th·ª≠ (X_test):  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VI.** M√£ h√≥a v√† Chu·∫©n h√≥a d·ªØ li·ªáu (Theo ƒë√∫ng quy tr√¨nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√°c ƒë·ªãnh c√°c c·ªôt c·∫ßn x·ª≠ l√Ω\n",
    "categorical_cols = ['category_code', 'brand']\n",
    "numerical_cols = ['price', 'hour', 'day', 'weekday']\n",
    "\n",
    "# T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh SettingWithCopyWarning\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "# 1. M√£ h√≥a c√°c c·ªôt categorical\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    # X·ª≠ l√Ω c√°c gi√° tr·ªã ch∆∞a t·ª´ng th·∫•y trong t·∫≠p test\n",
    "    X_test[col] = X_test[col].map(lambda s: -1 if s not in le.classes_ else le.transform([s])[0])\n",
    "\n",
    "# 2. Chu·∫©n h√≥a c√°c c·ªôt s·ªë\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(\"‚úì D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c m√£ h√≥a v√† chu·∫©n h√≥a theo ƒë√∫ng quy tr√¨nh.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VII.** C√¢n b·∫±ng d·ªØ li·ªáu hu·∫•n luy·ªán v·ªõi Oversampling (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ph√¢n b·ªë l·ªõp TR∆Ø·ªöC khi √°p d·ª•ng SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nPh√¢n b·ªë l·ªõp SAU khi √°p d·ª•ng SMOTE:\")\n",
    "print(y_train_smote.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VIII.** Hu·∫•n luy·ªán m√¥ h√¨nh Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  8.1. Ki·∫øn th·ª©c n·ªÅn t·∫£ng v·ªÅ Naive Bayes\n",
    "\n",
    "**Naive Bayes** l√† m·ªôt nh√≥m c√°c thu·∫≠t to√°n ph√¢n lo·∫°i d·ª±a tr√™n **ƒê·ªãnh l√Ω Bayes** v·ªõi m·ªôt gi·∫£ ƒë·ªãnh \"ng√¢y th∆°\" (naive) v·ªÅ s·ª± ƒë·ªôc l·∫≠p c√≥ ƒëi·ªÅu ki·ªán gi·ªØa c√°c ƒë·∫∑c tr∆∞ng.\n",
    "\n",
    "**C√¥ng th·ª©c Bayes c∆° b·∫£n:**\n",
    "$$P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)}$$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- $Y$: l√† bi·∫øn m·ª•c ti√™u (l·ªõp c·∫ßn d·ª± ƒëo√°n, v√≠ d·ª•: `mua` ho·∫∑c `kh√¥ng mua`).\n",
    "- $X$: l√† c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o (v√≠ d·ª•: `price`, `hour`, `brand`).\n",
    "- $P(Y|X)$: l√† x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán c·ªßa l·ªõp $Y$ khi bi·∫øt c√°c ƒë·∫∑c tr∆∞ng $X$ (ƒë√¢y l√† x√°c su·∫•t ch√∫ng ta mu·ªën t√¨m).\n",
    "- $P(X|Y)$: l√† x√°c su·∫•t xu·∫•t hi·ªán c√°c ƒë·∫∑c tr∆∞ng $X$ khi bi·∫øt l·ªõp l√† $Y$.\n",
    "\n",
    "**Gi·∫£ ƒë·ªãnh \"ng√¢y th∆°\"**: Thu·∫≠t to√°n gi·∫£ ƒë·ªãnh r·∫±ng t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng l√† ƒë·ªôc l·∫≠p v·ªõi nhau khi ƒë√£ bi·∫øt l·ªõp. ƒêi·ªÅu n√†y gi√∫p ƒë∆°n gi·∫£n h√≥a vi·ªác t√≠nh to√°n $P(X|Y)$ b·∫±ng c√°ch nh√¢n x√°c su·∫•t c·ªßa t·ª´ng ƒë·∫∑c tr∆∞ng ri√™ng l·∫ª.\n",
    "\n",
    "**Gaussian Naive Bayes**:\n",
    "Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta s·ª≠ d·ª•ng **Gaussian Naive Bayes** v√¨ c√°c ƒë·∫∑c tr∆∞ng s·ªë c·ªßa ch√∫ng ta (nh∆∞ `price`, `hour` sau khi chu·∫©n h√≥a) c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£ ƒë·ªãnh l√† tu√¢n theo **ph√¢n ph·ªëi chu·∫©n (Gaussian)**. Thu·∫≠t to√°n s·∫Ω t√≠nh to√°n gi√° tr·ªã trung b√¨nh (mean) v√† ph∆∞∆°ng sai (variance) cho t·ª´ng ƒë·∫∑c tr∆∞ng ·ª©ng v·ªõi m·ªói l·ªõp ƒë·ªÉ ∆∞·ªõc t√≠nh x√°c su·∫•t.\n",
    "$$P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c c√¢n b·∫±ng b·∫±ng SMOTE\n",
    "model_nb = GaussianNB()\n",
    "model_nb.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\"‚úÖ M√¥ h√¨nh Gaussian Naive Bayes ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán th√†nh c√¥ng.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IX.** ƒê√°nh gi√° m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê√°nh gi√° tr√™n t·∫≠p test g·ªëc ƒë·ªÉ ph·∫£n √°nh hi·ªáu nƒÉng th·ª±c t·∫ø\n",
    "y_pred = model_nb.predict(X_test)\n",
    "\n",
    "# 1. In b√°o c√°o ph√¢n lo·∫°i chi ti·∫øt cho m√¥ h√¨nh Naive Bayes\n",
    "print(\"üìä B√°o c√°o ph√¢n lo·∫°i chi ti·∫øt cho m√¥ h√¨nh Naive Bayes:\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **So s√°nh v·ªõi m√¥ h√¨nh d·ª± ƒëo√°n ng·∫´u nhi√™n**\n",
    "\n",
    "ƒê·ªÉ so s√°nh m·ªôt c√°ch c√¥ng b·∫±ng, ch√∫ng ta t·∫°o ra m·ªôt m√¥ h√¨nh d·ª± ƒëo√°n ng·∫´u nhi√™n nh∆∞ng c√≥ c√πng **t·ª∑ l·ªá d·ª± ƒëo√°n \"mua\"** nh∆∞ m√¥ h√¨nh Naive Bayes. ƒêi·ªÅu n√†y gi√∫p ki·ªÉm tra xem m√¥ h√¨nh c·ªßa ch√∫ng ta c√≥ th·ª±c s·ª± \"th√¥ng minh\" h∆°n vi·ªác ƒëo√°n ng·∫´u nhi√™n v·ªõi c√πng m·ªôt t·∫ßn su·∫•t hay kh√¥ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh t·ª∑ l·ªá d·ª± ƒëo√°n mua h√†ng c·ªßa m√¥ h√¨nh Naive Bayes\n",
    "nb_prediction_rate = np.mean(y_pred)\n",
    "print(f\"T·ª∑ l·ªá d·ª± ƒëo√°n 'Mua' c·ªßa m√¥ h√¨nh Naive Bayes: {nb_prediction_rate:.4f}\")\n",
    "\n",
    "# T·∫°o d·ª± ƒëo√°n ng·∫´u nhi√™n v·ªõi c√πng t·ª∑ l·ªá tr√™n\n",
    "np.random.seed(42) # ƒê·ªÉ ƒë·∫£m b·∫£o k·∫øt qu·∫£ c√≥ th·ªÉ t√°i l·∫≠p\n",
    "y_pred_random = np.random.choice([0, 1], size=len(y_test), p=[1 - nb_prediction_rate, nb_prediction_rate])\n",
    "\n",
    "print(\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cho m√¥ h√¨nh d·ª± ƒëo√°n ng·∫´u nhi√™n:\\n\")\n",
    "print(classification_report(y_test, y_pred_random))\n",
    "\n",
    "# L·∫•y ma tr·∫≠n nh·∫ßm l·∫´n cho c·∫£ hai m√¥ h√¨nh\n",
    "cm_nb = confusion_matrix(y_test, y_pred)\n",
    "cm_random = confusion_matrix(y_test, y_pred_random)\n",
    "\n",
    "# V·∫Ω hai ma tr·∫≠n nh·∫ßm l·∫´n c·∫°nh nhau ƒë·ªÉ so s√°nh\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Ma tr·∫≠n c·ªßa Naive Bayes\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0],\n",
    "            xticklabels=['Kh√¥ng mua', 'Mua'], yticklabels=['Kh√¥ng mua', 'Mua'])\n",
    "axes[0].set_title(\"üîç Confusion Matrix - Naive Bayes\")\n",
    "axes[0].set_xlabel(\"D·ª± ƒëo√°n\")\n",
    "axes[0].set_ylabel(\"Th·ª±c t·∫ø\")\n",
    "\n",
    "# Ma tr·∫≠n c·ªßa m√¥ h√¨nh ng·∫´u nhi√™n\n",
    "sns.heatmap(cm_random, annot=True, fmt='d', cmap='Greens', cbar=False, ax=axes[1],\n",
    "            xticklabels=['Kh√¥ng mua', 'Mua'], yticklabels=['Kh√¥ng mua', 'Mua'])\n",
    "axes[1].set_title(\"üé≤ Confusion Matrix - D·ª± ƒëo√°n ng·∫´u nhi√™n\")\n",
    "axes[1].set_xlabel(\"D·ª± ƒëo√°n\")\n",
    "axes[1].set_ylabel(\"Th·ª±c t·∫ø\")\n",
    "\n",
    "plt.suptitle('So s√°nh hi·ªáu su·∫•t m√¥ h√¨nh', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **X.** T·ªïng k·∫øt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **T√≥m t·∫Øt k·∫øt qu·∫£:**\n",
    "1.  **V·∫•n ƒë·ªÅ ban ƒë·∫ßu:** D·ªØ li·ªáu b·ªã m·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng v·ªõi l·ªõp 'mua h√†ng' (l·ªõp 1) ch·ªâ chi·∫øm ~1.4%. N·∫øu kh√¥ng x·ª≠ l√Ω, m√¥ h√¨nh s·∫Ω c√≥ xu h∆∞·ªõng b·ªè qua l·ªõp n√†y.\n",
    "\n",
    "2.  **Gi·∫£i ph√°p:** Ch√∫ng ta ƒë√£ √°p d·ª•ng th√†nh c√¥ng k·ªπ thu·∫≠t **SMOTE (Oversampling)** tr√™n t·∫≠p hu·∫•n luy·ªán ƒë·ªÉ t·∫°o ra c√°c m·∫´u t·ªïng h·ª£p, gi√∫p c√¢n b·∫±ng s·ªë l∆∞·ª£ng gi·ªØa hai l·ªõp.\n",
    "\n",
    "3.  **K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh Naive Bayes:**\n",
    "    *   **Recall (L·ªõp 1 - Mua h√†ng):** Ch·ªâ s·ªë n√†y ƒë√£ tƒÉng l√™n m·ªôt m·ª©c ƒë√°ng k·ªÉ, cho th·∫•y m√¥ h√¨nh ƒë√£ c√≥ kh·∫£ nƒÉng **ph√°t hi·ªán ƒë∆∞·ª£c m·ªôt ph·∫ßn** c√°c tr∆∞·ªùng h·ª£p kh√°ch h√†ng th·ª±c s·ª± mua h√†ng, thay v√¨ b·ªè s√≥t ho√†n to√†n.\n",
    "    *   **Precision (L·ªõp 1 - Mua h√†ng):** Ch·ªâ s·ªë n√†y kh√¥ng cao, cho th·∫•y m√¥ h√¨nh v·∫´n c√≤n d·ª± ƒëo√°n sai kh√° nhi·ªÅu (d·ª± ƒëo√°n \"mua\" nh∆∞ng th·ª±c t·∫ø l√† \"kh√¥ng mua\"). ƒê√¢y l√† s·ª± ƒë√°nh ƒë·ªïi th∆∞·ªùng th·∫•y ƒë·ªÉ c√≥ ƒë∆∞·ª£c Recall cao h∆°n.\n",
    "    *   **So s√°nh Confusion Matrix:** Nh√¨n v√†o hai ma tr·∫≠n nh·∫ßm l·∫´n, ta th·∫•y r√µ s·ª± kh√°c bi·ªát. M·∫∑c d√π c·∫£ hai m√¥ h√¨nh c√≥ th·ªÉ d·ª± ƒëo√°n \"mua\" v·ªõi s·ªë l∆∞·ª£ng t∆∞∆°ng ƒë∆∞∆°ng (t·ªïng c·ªôt 'Mua' g·∫ßn b·∫±ng nhau), nh∆∞ng m√¥ h√¨nh Naive Bayes ph√¢n b·ªï c√°c d·ª± ƒëo√°n ƒë√≥ m·ªôt c√°ch \"th√¥ng minh\" h∆°n nhi·ªÅu. C·ª• th·ªÉ, s·ªë l∆∞·ª£ng **True Positives** (d·ª± ƒëo√°n ƒë√∫ng l√† 'Mua') c·ªßa Naive Bayes cao h∆°n ƒë√°ng k·ªÉ so v·ªõi m√¥ h√¨nh ng·∫´u nhi√™n, trong khi s·ªë l∆∞·ª£ng **False Positives** (d·ª± ƒëo√°n sai l√† 'Mua') l·∫°i th·∫•p h∆°n.\n",
    "\n",
    "### **K·∫øt lu·∫≠n:**\n",
    "Vi·ªác √°p d·ª•ng **Oversampling (SMOTE)** ƒë√£ ch·ª©ng t·ªè hi·ªáu qu·∫£ trong vi·ªác gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu. M√¥ h√¨nh Naive Bayes, sau khi ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu c√¢n b·∫±ng, ƒë√£ tr·ªü n√™n h·ªØu √≠ch h∆°n trong th·ª±c t·∫ø v√¨ n√≥ c√≥ kh·∫£ nƒÉng x√°c ƒë·ªãnh c√°c kh√°ch h√†ng ti·ªÅm nƒÉng, d√π ph·∫£i ƒë√°nh ƒë·ªïi m·ªôt ch√∫t v·ªÅ ƒë·ªô ch√≠nh x√°c c·ªßa c√°c d·ª± ƒëo√°n n√†y.\n",
    "\n",
    "**Vi·ªác so s√°nh tr·ª±c ti·∫øp v·ªõi m·ªôt m√¥ h√¨nh d·ª± ƒëo√°n ng·∫´u nhi√™n c√≥ c√πng t·ª∑ l·ªá ƒë·∫ßu ra** ƒë√£ kh·∫≥ng ƒë·ªãnh m·ªôt c√°ch m·∫°nh m·∫Ω r·∫±ng m√¥ h√¨nh Naive Bayes ƒë√£ th·ª±c s·ª± h·ªçc ƒë∆∞·ª£c c√°c m·∫´u (patterns) c√≥ √Ω nghƒ©a t·ª´ d·ªØ li·ªáu. N√≥ kh√¥ng ch·ªâ ƒë∆°n thu·∫ßn ƒë∆∞a ra d·ª± ƒëo√°n m√† c√≤n ƒë∆∞a ra nh·ªØng d·ª± ƒëo√°n c√≥ ƒë·ªô ch√≠nh x√°c cao h∆°n h·∫≥n so v·ªõi vi·ªác tung ƒë·ªìng xu. ƒêi·ªÅu n√†y ch·ª©ng minh gi√° tr·ªã c·ªßa vi·ªác x√¢y d·ª±ng m√¥ h√¨nh h·ªçc m√°y trong b√†i to√°n n√†y."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
